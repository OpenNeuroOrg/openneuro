# docker compose versions
version: '2.3'

# shared volumes
volumes:
  project:
  yarn_cache:
  node_modules:
  webpack_cache:

services:
  # This dummy service provides shared configuration for all Node deps
  nodejs:
    image: node:12
    env_file: ./config.env
    working_dir: /srv
    volumes:
      - .:/srv
      - yarn_cache:/root/.cache
      - node_modules:/srv/node_modules
      - project:/srv/packages/openneuro-app/dist
    tmpfs:
      - /srv/packages/openneuro-app/node_modules:exec
      - /srv/packages/openneuro-client/node_modules:exec
      - /srv/packages/openneuro-cli/node_modules:exec
      - /srv/packages/openneuro-server/node_modules:exec
      - /srv/packages/openneuro-indexer/node_modules:exec

  # web app bundle build
  app:
    extends:
      service: nodejs
    working_dir: /srv/packages/openneuro-app
    command: sh -c "yarn install --mutex file:/srv/node_modules/.yarn-mutex && yarn start"
    volumes:
      - webpack_cache:/webpack-cache
    ports:
      - '8145:8145'

  # crn node server
  server:
    extends:
      service: nodejs
    command: sh -c "yarn install --mutex file:/srv/node_modules/.yarn-mutex && cd /srv/packages/openneuro-server && node index.js"
    depends_on:
      - redis
      - mongo
      - datalad
      - elasticsearch

  # Elastic Search indexer
  indexer:
    extends:
      service: nodejs
    command: sh -c "yarn install --mutex file:/srv/node_modules/.yarn-mutex && yarn ts-node /srv/packages/openneuro-indexer/src/index.ts"
    depends_on:
      - server
      - elasticsearch
    extends:
      service: nodejs

  content:
    image: ${CONTENT_IMAGE}
    volumes:
      - /content

  # mongodb
  mongo:
    image: mongo:4.0
    volumes:
      - ${PERSISTENT_DIR}/mongo:/data/db

  # Redis
  redis:
    image: redis:alpine
    volumes:
      - ${PERSISTENT_DIR}/redis:/data

  # datalad Python backend
  datalad:
    image: openneuro/datalad-service:latest
    volumes:
      - ${PERSISTENT_DIR}/datalad:/datalad
      - ./services/datalad:/datalad_service
    env_file: ./config.env
    depends_on:
      - celery
    init: true

  # celery Python backend
  celery:
    image: openneuro/datalad-service:latest
    command:
      - /dataset-worker
    scale: 2
    volumes:
      - ${PERSISTENT_DIR}/datalad:/datalad
      - ./services/datalad:/datalad_service
    env_file: ./config.env
    init: true

  # publish-only celery worker
  publish:
    image: openneuro/datalad-service:latest
    command:
      - /publish-worker
    volumes:
      - ${PERSISTENT_DIR}/datalad:/datalad
      - ./services/datalad:/datalad_service
      - ./datalad-key:/datalad-key
    env_file: ./config.env
    init: true

  flower:
    image: openneuro/datalad-service:latest
    command:
      - flower
      - -A
      - datalad_service.worker
      - --broker
      - redis://redis
    env_file: ./config.env
    ports:
      - '5555:5555'

  # nginx + app
  web:
    image: nginx:1.16.1
    volumes:
      - ./nginx/nginx.dev.conf:/etc/nginx/conf.d/default.conf:ro
    volumes_from:
      - content
    ports:
      - '80:80'
      - '8110:8110'
      - '9876:80'
    depends_on:
      - server
      - datalad

  elasticsearch:
    image: elasticsearch:7.5.1
    environment:
      discovery.type: single-node
      cluster.routing.allocation.disk.threshold_enabled: 'true'
      cluster.routing.allocation.disk.watermark.flood_stage: 1gb
      cluster.routing.allocation.disk.watermark.low: 10gb
      cluster.routing.allocation.disk.watermark.high: 5gb
    ports:
      - '9200:9200'
      - '9300:9300'
